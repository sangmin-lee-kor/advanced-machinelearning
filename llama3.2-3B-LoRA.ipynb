{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shbs1113/miniconda3/envs/llm_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, LlamaForCausalLM, LlamaTokenizer, DataCollatorForSeq2Seq\n",
    "from peft import get_peft_config, LoraConfig, get_peft_model, get_peft_model_state_dict\n",
    "import torch\n",
    "import sys\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/data params\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "data_path = 'train_data.json'\n",
    "output_path = \"mycheckpoint\"\n",
    "# training hyperparams\n",
    "batch_size = 128#used to be 128\n",
    "micro_batch_size = 4\n",
    "num_epochs = 3\n",
    "learning_rate = 3e-4\n",
    "cutoff_len = 256\n",
    "val_set_size = 0\n",
    "# lora hyperparams\n",
    "lora_r= 16\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "# llm hyperparams\n",
    "train_on_inputs = True  # if False, masks out inputs in loss\n",
    "group_by_length = False  # faster, but produces an odd training loss curve\n",
    "# wandb params\n",
    "wandb_project= \"\"\n",
    "wandb_run_name= \"\"\n",
    "wandb_watch = \"\"  # options: false | gradients | all\n",
    "wandb_log_model = \"\"  # options: false | true\n",
    "resume_from_checkpoint = None  # either training checkpoint or final adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_TOKEN=\"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\" ## hugging face access token ÏûÖÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/shbs1113/miniconda3/envs/llm_env/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ÏÇ¨Ï†Ñ ÌïôÏäµ Î™®Îç∏ Î°úÎìú\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16, use_auth_token=ACCESS_TOKEN\n",
    "    )\n",
    "# The problem is that tokenizer is expecting a local file path. \n",
    "# This can be fixed by specifying the 'tokenizer_class' \n",
    "# and removing the 'add_eos_token' for this specific model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    use_auth_token=ACCESS_TOKEN, \n",
    "    tokenizer_class=LlamaTokenizer,\n",
    ")  \n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Ï§ÄÎπÑ (ÏòàÏãú Îç∞Ïù¥ÌÑ∞)\n",
    "data = [\n",
    "    {\"input\": \"Question: What is AI?\", \"output\": \"AI stands for Artificial Intelligence.\"},\n",
    "    {\"input\": \"Explain machine learning.\", \"output\": \"Machine learning is a subset of AI.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,587,520 || all params: 3,217,337,344 || trainable%: 0.1426\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=lora_r,                      # Low-rank Ï∞®Ïõê\n",
    "    lora_alpha=lora_alpha,            # Scaling factor\n",
    "    target_modules=lora_target_modules,  # Ï†ÅÏö©Ìï† Î™®Îìà (Î™®Îç∏ Íµ¨Ï°∞Ïóê Îî∞Îùº Îã§Î¶Ñ)\n",
    "    lora_dropout=lora_dropout,         # Dropout ÎπÑÏú®\n",
    "    bias=\"none\",              # Bias Ï≤òÎ¶¨ Î∞©Ïãù\n",
    "    task_type=\"CAUSAL_LM\"     # ÏûëÏóÖ Ïú†Ìòï (Ïòà: CAUSAL_LM, SEQ_2_SEQ_LM Îì±)\n",
    ")\n",
    "\n",
    "# LoRAÎ•º Î™®Îç∏Ïóê Ï†ÅÏö©\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # Trainable parameter ÌôïÏù∏\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "enerating train split: 610 examples [00:00, 3618.58 examples/s]"
     ]
    }
   ],
   "source": [
    "data_set = load_dataset(\"json\", data_files=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, tokenizer, cutoff_len, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_prompt(data_point) :\n",
    "    if data_point[\"input\"] :\n",
    "        prompt = \"{instruction}  \\ninput : {input}  \\noutput : {output}\".format(instruction=data_point[\"instruction\"], input=data_point[\"input\"], output=data_point[\"output\"])\n",
    "        return prompt\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point, tokenizer, cutoff_len):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt, tokenizer, cutoff_len)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = generate_prompt({**data_point, \"output\": \"\"})\n",
    "        tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 610/610 [00:02<00:00, 234.44 examples/s]"
     ]
    }
   ],
   "source": [
    "split_data = data_set.shuffle().map(lambda prompt: generate_and_tokenize_prompt(prompt, tokenizer, cutoff_len))\n",
    "train_data = split_data[\"train\"]\n",
    "validation_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shbs1113/miniconda3/envs/llm_env/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=micro_batch_size,\n",
    "        gradient_accumulation_steps=batch_size // micro_batch_size,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_torch\",\n",
    "        evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=200 if val_set_size > 0 else None,\n",
    "        save_steps=200,\n",
    "        output_dir=output_path,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "        group_by_length=group_by_length,\n",
    "        ),\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )\n",
    ").__get__(model, type(model))\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 02:34, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.928200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shbs1113/miniconda3/envs/llm_env/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67551502-086d07c07184c1716590d0f0;cc72d9eb-ae3a-4373-813a-f576fd8a8232)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-3B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/shbs1113/miniconda3/envs/llm_env/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/shbs1113/miniconda3/envs/llm_env/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67551502-762d95ae1f52eb3679486729;c0208378-4084-412d-87ee-99c5469c7a03)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-3B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=1.8688889543215434, metrics={'train_runtime': 169.7165, 'train_samples_per_second': 10.783, 'train_steps_per_second': 0.071, 'total_flos': 6661082764541952.0, 'train_loss': 1.8688889543215434, 'epoch': 2.810457516339869})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shbs1113/miniconda3/envs/llm_env/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67551503-70fb573e58451daf224643c5;f264998f-9ce3-4ebe-a471-cfb9ab43f57c)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-3B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/shbs1113/miniconda3/envs/llm_env/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/shbs1113/miniconda3/envs/llm_env/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67551503-2128c2316727c0bf021bc0ad;a4814eb2-9770-44be-820a-0dfd6d0e5664)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-3B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./final_model/tokenizer_config.json',\n",
       " './final_model/special_tokens_map.json',\n",
       " './final_model/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
